{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77a9d33a-68af-446e-9630-1b0656ef7844",
      "metadata": {
        "id": "77a9d33a-68af-446e-9630-1b0656ef7844"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54862a76-b52d-49f7-8daf-bead24f421c7",
      "metadata": {
        "id": "54862a76-b52d-49f7-8daf-bead24f421c7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "\n",
        "# NLTK Packages\n",
        "import nltk\n",
        "#nltk.download( 'vader_lexicon' )\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "# Spacy Packages\n",
        "import spacy\n",
        "#from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "\n",
        "# Models used for classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "# Feature Extractor for text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "#import skipthoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "29f4e008-5218-4aaa-9fe2-e28bd705d154",
      "metadata": {
        "id": "29f4e008-5218-4aaa-9fe2-e28bd705d154"
      },
      "outputs": [],
      "source": [
        "## Import Data\n",
        "#C:\\Users\\coope\\Data_Science\\Datasets\\Yelp\\Json\n",
        "#C:\\Users\\coope\\Data_Science\\Datasets\\Yelp\\Json\\yelp_academic_dataset_business.json\n",
        "input_path_business_info = 'C:\\\\Users\\\\coope\\\\Data_Science\\\\Datasets\\\\Yelp\\\\Json\\\\yelp_academic_dataset_business.json'\n",
        "input_path_review_data = 'C:\\\\Users\\\\coope\\\\Data_Science\\\\Datasets\\\\Yelp\\\\Json\\\\yelp_academic_dataset_review.json'\n",
        "output_path_business_id = 'C:\\\\Users\\\\coope\\\\Data_Science\\\\Datasets\\\\Yelp\\\\business_id.csv'\n",
        "output_path_total_data = 'C:\\\\Users\\\\coope\\\\Data_Science\\\\Datasets\\\\Yelp\\\\total_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "10e5a6dd-427e-4cd5-b5e2-4768a633f3a7",
      "metadata": {
        "id": "10e5a6dd-427e-4cd5-b5e2-4768a633f3a7",
        "outputId": "dbe345b4-9335-48cd-93c5-18df1c874e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-748fc14c9b45>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path_business_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows can only be passed if lines=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         ):\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File C:\\Users\\coope\\Data_Science\\Datasets\\Yelp\\Json\\yelp_academic_dataset_business.json does not exist"
          ]
        }
      ],
      "source": [
        "df = pd.read_json(input_path_business_info, lines = True)\n",
        "\n",
        "df = df.fillna(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdcfdcc-0cd2-4eeb-ae92-89af82f990bf",
      "metadata": {
        "id": "cbdcfdcc-0cd2-4eeb-ae92-89af82f990bf"
      },
      "outputs": [],
      "source": [
        "resturants = df[df[\"categories\"].str.contains(\"Restaurants\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939c5857-a464-4112-ba57-5b5de8533f18",
      "metadata": {
        "id": "939c5857-a464-4112-ba57-5b5de8533f18"
      },
      "outputs": [],
      "source": [
        "res_id = resturants[\"business_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb400f67-d122-41f0-9d60-167c9f6a95bc",
      "metadata": {
        "id": "bb400f67-d122-41f0-9d60-167c9f6a95bc"
      },
      "outputs": [],
      "source": [
        "chunks = pd.read_json(path_review, lines = True, chunksize = 10000)\n",
        "#i = 0\n",
        "total_data = pd.DataFrame()\n",
        "\n",
        "for chunk in chunks:\n",
        "    #if i >= 500:\n",
        "        #break\n",
        "    #i += 1\n",
        "    resturant = chunk[chunk[\"business_id\"].isin(res_id)][[\"stars\", \"text\"]]\n",
        "    total_data = pd.concat([total_data, resturant])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ddfc61-51b9-420a-840b-e36cbb45e523",
      "metadata": {
        "id": "d6ddfc61-51b9-420a-840b-e36cbb45e523"
      },
      "outputs": [],
      "source": [
        "total_data[\"text\"][0]\n",
        "total_data[\"stars\"][0]\n",
        "\n",
        "total_data.to_csv(output_path_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62800780-2bed-474f-8b33-dffdceb42aed",
      "metadata": {
        "id": "62800780-2bed-474f-8b33-dffdceb42aed"
      },
      "source": [
        "# Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e673a4c2-f41b-4837-ab4e-f3bce0d203e8",
      "metadata": {
        "id": "e673a4c2-f41b-4837-ab4e-f3bce0d203e8"
      },
      "outputs": [],
      "source": [
        "path = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\yelp_dataset\\\\total_data.csv'\n",
        "out_path_binary = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\Raw Data\\\\Yelp Data\\\\binary.csv'\n",
        "out_path_nominal = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\Raw Data\\\\Yelp Data\\\\nominal.csv'\n",
        "\n",
        "df = pd.read_csv(path, index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed542e1-2393-435f-bdee-8abe814f1dc0",
      "metadata": {
        "id": "aed542e1-2393-435f-bdee-8abe814f1dc0"
      },
      "outputs": [],
      "source": [
        "plt.hist(df[\"stars\"])\n",
        "plt.show()\n",
        "\n",
        "pd.crosstab(index = df[\"stars\"], columns = \"prop\") / pd.crosstab(index = df[\"stars\"], columns = \"prop\").sum()\n",
        "pd.crosstab(index = df[\"stars\"], columns = \"prop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef13a2ff-559c-4745-b793-ab3859f72b70",
      "metadata": {
        "id": "ef13a2ff-559c-4745-b793-ab3859f72b70"
      },
      "outputs": [],
      "source": [
        "# Data set under assumption that 1, 2, and 3 are bad, 4 and 5 are good\n",
        "df[\"binary\"] = [\"Positive\" if x >= 4 else \"Negative\" for x in df[\"stars\"]]\n",
        "#binary\n",
        "\n",
        "pd.crosstab(index = df[\"binary\"], columns = \"prop\") / pd.crosstab(index = df[\"binary\"], columns = \"prop\").sum()\n",
        "df.to_csv(out_path_binary, columns = [\"text\", \"binary\"], index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08c6dfa-5242-4e67-a3f8-d6cb0d1af9ee",
      "metadata": {
        "id": "a08c6dfa-5242-4e67-a3f8-d6cb0d1af9ee"
      },
      "outputs": [],
      "source": [
        "# Data set under assumption that 1 and 2 are bad, 4 and 5 are good, and 3 is Neutral\n",
        "df[\"nominal\"] = [\"Positive\" if x >= 4 else \"Negative\" if x <= 2 else \"Neutral\" for x in df[\"stars\"]]\n",
        "pd.crosstab(index = df[\"nominal\"], columns = \"prop\") / pd.crosstab(index = df[\"nominal\"], columns = \"prop\").sum()\n",
        "\n",
        "df.to_csv(out_path_nominal, columns = [\"text\", \"nominal\"], index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50bb680-8055-4ee2-a472-96bfca4cdd74",
      "metadata": {
        "id": "d50bb680-8055-4ee2-a472-96bfca4cdd74"
      },
      "source": [
        "# Text Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebd7c03-30bd-4908-bc5b-a7c81aea434c",
      "metadata": {
        "id": "0ebd7c03-30bd-4908-bc5b-a7c81aea434c"
      },
      "outputs": [],
      "source": [
        "input_path_nominal = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\Raw Data\\\\Yelp Data\\\\nominal.csv'\n",
        "input_path_binary = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\Raw Data\\\\Yelp Data\\\\binary.csv'\n",
        "path = 'C:\\\\Users\\\\coope\\\\OneDrive\\\\Desktop\\\\Side_Projects\\\\yelp_dataset\\\\total_data.csv'\n",
        "\n",
        "df = pd.read_csv(path, index_col = 0)\n",
        "#df = pd.read_csv(input_path_binary)\n",
        "#df = pd.read_csv(input_path_nominal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e5e91e-7aa2-4fe9-896e-e9c93dd312cc",
      "metadata": {
        "id": "22e5e91e-7aa2-4fe9-896e-e9c93dd312cc"
      },
      "outputs": [],
      "source": [
        "df_sample = df.sample(20000, random_state = 12345)\n",
        "\n",
        "#pd.crosstab(index = df[\"nominal\"], columns = \"prop\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( df_sample[\"text\"], df_sample[\"stars\"], test_size = 0.30, random_state = 12345)\n",
        "#X_train, X_test, y_train, y_test = train_test_split( df_sample[\"text\"], df_sample[\"nominal\"], test_size = 0.30, random_state = 12345)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc0509b-c99e-4aa8-a83a-e26c8edab1d7",
      "metadata": {
        "id": "2cc0509b-c99e-4aa8-a83a-e26c8edab1d7"
      },
      "outputs": [],
      "source": [
        "# TF-IDF bag of n-grams pipeline and logistic regression\n",
        "pipeline_tfidf_LOG = Pipeline(steps = [('vectorizer', TfidfVectorizer(ngram_range = (1, 2))),\n",
        "                        ('classifier', LogisticRegression())])\n",
        "\n",
        "\n",
        "pipeline_tfidf_LOG.fit(X_train, y_train)\n",
        "pipeline_tfidf_LOG.score(X_test, y_test) # 62.017 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75877077-59de-4313-b076-75d7b1638d61",
      "metadata": {
        "id": "75877077-59de-4313-b076-75d7b1638d61"
      },
      "outputs": [],
      "source": [
        "# TF-IDF bag of n-grams pipeline and support vector machine (Linear)\n",
        "pipeline_tfidf_SVM_L = Pipeline(steps = [('vectorizer', TfidfVectorizer(ngram_range = (1, 2))),\n",
        "                        ('classifier', svm.LinearSVC())])\n",
        "\n",
        "\n",
        "pipeline_tfidf_SVM_L.fit(X_train, y_train)\n",
        "pipeline_tfidf_SVM_L.score(X_test, y_test) # 63.067%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb8b926-69c4-4d79-a648-4d2525b3be38",
      "metadata": {
        "id": "deb8b926-69c4-4d79-a648-4d2525b3be38"
      },
      "outputs": [],
      "source": [
        "# TF-IDF bag of n-grams pipeline and Multinomial Naive Bayes\n",
        "pipeline_tfidf_MNB = Pipeline(steps = [('vectorizer', TfidfVectorizer(ngram_range = (1, 2))),\n",
        "                        ('classifier', MultinomialNB())])\n",
        "\n",
        "\n",
        "pipeline_tfidf_MNB.fit(X_train, y_train)\n",
        "pipeline_tfidf_MNB.score(X_test, y_test) # 42.883%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50880aea-ebea-42c2-a381-5c395cc51c63",
      "metadata": {
        "id": "50880aea-ebea-42c2-a381-5c395cc51c63"
      },
      "outputs": [],
      "source": [
        "# TF-IDF bag of n-grams pipeline and Complement Naive Bayes\n",
        "pipeline_tfidf_CNB = Pipeline(steps = [('vectorizer', TfidfVectorizer(ngram_range = (1, 2))),\n",
        "                        ('classifier', ComplementNB())])\n",
        "\n",
        "\n",
        "pipeline_tfidf_CNB.fit(X_train, y_train)\n",
        "pipeline_tfidf_CNB.score(X_test, y_test) # 45.467%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}